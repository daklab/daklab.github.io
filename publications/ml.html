<ol>
<li><div id="Stirn2019"><em>Stirn A, Jebara T and Knowles DA</em> (2019), <strong>"A New Distribution on the Simplex with Auto-Encoding Applications"</strong>, In Advances in Neural Information Processing Systems. </div>
<div>[<a data-toggle="collapse" href="#abs_Stirn2019">Abstract</a>][<a href="#bib_Stirn2019" data-toggle="collapse">BibTeX</a>] [<a href="http://papers.nips.cc/paper/9520-a-new-distribution-on-the-simplex-with-auto-encoding-applications" target="_blank">URL</a>]</div>

<div id="abs_Stirn2019" class="collapse">
	<b>Abstract</b>: We construct a new distribution for the simplex using the Kumaraswamy distribution and an ordered stick-breaking process. We explore and develop the theoretical properties of this new distribution and prove that it exhibits symmetry (exchangeability) under the same conditions as the well-known Dirichlet. Like the Dirichlet, the new distribution is adept at capturing sparsity but, unlike the Dirichlet, has an exact and closed form reparameterization--making it well suited for deep variational Bayesian modeling. We demonstrate the distribution's utility in a variety of semi-supervised auto-encoding tasks. In all cases, the resulting models achieve competitive performance commensurate with their simplicity, use of explicit probability models, and abstinence from adversarial training.
</div>
<div id="bib_Stirn2019" class="collapse">
<b>BibTeX</b>:
<pre>
@inproceedings{Stirn2019,
  author = {Andrew Stirn and Tony Jebara and David A. Knowles},
  title = {A New Distribution on the Simplex with Auto-Encoding Applications},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2019},
  url = {http://papers.nips.cc/paper/9520-a-new-distribution-on-the-simplex-with-auto-encoding-applications}
}
</pre>
</div>
</li><li><div id="palla2017bdfp"><em>Palla* K, Knowles* DA and Ghahramani Z</em> (2017), <strong>"A birth-death process for feature allocation."</strong>, In Proceedings of the 34th International Conference on Machine Learning.  *These authors contributed equally to this work.</div>
<div>[<a data-toggle="collapse" href="#abs_palla2017bdfp">Abstract</a>][<a href="#bib_palla2017bdfp" data-toggle="collapse">BibTeX</a>]</div>

<div id="abs_palla2017bdfp" class="collapse">
	<b>Abstract</b>: We propose a Bayesian nonparametric prior over feature allocations for sequential data, the birth-death feature allocation process (BDFP). The BDFP models the evolution of the feature allocation of a set of N objects across a covariate (e.g.time) by creating and deleting features. A BDFP is exchangeable, projective, stationary and reversible, and its equilibrium distribution is given by the Indian buffet process (IBP). We also show that the Beta process on an extended space is the de Finetti mixing distribution underlying the BDFP. Finally, we present the finite approximation of the BDFP, the Beta Event Process (BEP), that permits simplified inference. The utility of the BDFP as a prior is demonstrated on real world dynamic genomics and social network data.
</div>
<div id="bib_palla2017bdfp" class="collapse">
<b>BibTeX</b>:
<pre>
@inproceedings{palla2017bdfp,
  author = {Palla*, Konstantina and Knowles*, David A. and Ghahramani, Zoubin},
  title = {A birth-death process for feature allocation.},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  year = {2017}
}
</pre>
</div>
</li><li><div id="Knowles2014"><em>Knowles DA and Ghahramani Z</em> (2015), <strong>"Pitman Yor Diffusion Trees for Bayesian hierarchical clustering"</strong>, IEEE Transactions on Pattern Analysis and Machine Intelligence.  Vol. 37(2), pp. 271-289.</div>
<div>[<a data-toggle="collapse" href="#abs_Knowles2014">Abstract</a>][<a href="#bib_Knowles2014" data-toggle="collapse">BibTeX</a>] [<a href="https://doi.org/10.1109/TPAMI.2014.2313115" target="_blank">DOI</a>] [<a href="https://dx.doi.org/10.1109/TPAMI.2014.2313115" target="_blank">URL</a>] [<a href="//cs.stanford.edu/people/davidknowles/knowles_tpami2014.pdf" target="_blank">PDF</a>] [<a href="alberto.pdf" target="_blank">Slides</a>]</div>

<div id="abs_Knowles2014" class="collapse">
	<b>Abstract</b>: In this paper we introduce the Pitman Yor Diffusion Tree (PYDT), a Bayesian non-parametric prior over tree structures which generalises the Dirichlet Diffusion Tree [Neal, 2001] and removes the restriction to binary branching structure. The generative process is described and shown to result in an exchangeable distribution over data points. We prove some theoretical properties of the model including showing its construction as the continuum limit of a nested Chinese restaurant process model. We then present two alternative MCMC samplers which allows us to model uncertainty over tree structures, and a computationally efficient greedy Bayesian EM search algorithm. Both algorithms use message passing on the tree structure. The utility of the model and algorithms is demonstrated on synthetic and real world data, both continuous and binary.
</div>
<div id="bib_Knowles2014" class="collapse">
<b>BibTeX</b>:
<pre>
@article{Knowles2014,
  author = {Knowles, David A. and Ghahramani, Zoubin},
  title = {Pitman Yor Diffusion Trees for Bayesian hierarchical clustering},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {IEEE Computer Society},
  year = {2015},
  volume = {37},
  number = {2},
  pages = {271--289},
  url = {https://dx.doi.org/10.1109/TPAMI.2014.2313115},
  doi = {10.1109/TPAMI.2014.2313115}
}
</pre>
</div>
</li><li><div id="nguyen2015using"><em>Nguyen K, Bredno J and Knowles DA</em> (2015), <strong>"Using contextual information to classify nuclei in histology images"</strong>, In IEEE 12th International Symposium on Biomedical Imaging (ISBI). , pp. 995-998.</div>
<div>[<a data-toggle="collapse" href="#abs_nguyen2015using">Abstract</a>][<a href="#bib_nguyen2015using" data-toggle="collapse">BibTeX</a>] [<a href="http://dx.doi.org/10.1109/ISBI.2015.7164038" target="_blank">URL</a>] [<a href="nguyen_contextual.pdf" target="_blank">PDF</a>]</div>

<div id="abs_nguyen2015using" class="collapse">
	<b>Abstract</b>: Nucleus classification is a central task in digital pathology. Given a tissue image, our goal is to classify detected nuclei into different types, for example nuclei of tumor cells, stroma cells, or immune cells. State-of-the-art methods achieve this by extracting different types of features such as morphology, image intensities, and texture features in the nucleus regions. Such features are input to training and classification, e.g. using a support vector machine. In this paper, we introduce additional contextual information obtained from neighboring nuclei or texture in the surrounding tissue regions to improve nucleus classification. Three different methods are presented. These methods use conditional random fields (CRF), texture features computed in image patches centered at each nucleus, and a novel method based on the bag-of-word (BoW) model. The methods are evaluated on images of tumor-burdened tissue from H&amp;E-stained and Ki-67-stained breast samples. The experimental results show that contextual information systematically improves classification accuracy. The proposed BoW-based method performs better than the CRF-based method, and requires less computation than the texture-feature-based method.
</div>
<div id="bib_nguyen2015using" class="collapse">
<b>BibTeX</b>:
<pre>
@inproceedings{nguyen2015using,
  author = {Nguyen, Kien and Bredno, Joerg and Knowles, David A},
  title = {Using contextual information to classify nuclei in histology images},
  booktitle = {IEEE 12th International Symposium on Biomedical Imaging (ISBI)},
  year = {2015},
  pages = {995--998},
  url = {http://dx.doi.org/10.1109/ISBI.2015.7164038}
}
</pre>
</div>
</li><li><div id="Palla2015"><em>Palla K, Knowles DA and Ghahramani Z</em> (2015), <strong>"Relational learning and network modelling using infinite latent attribute models"</strong>, IEEE Transactions on Pattern Analysis and Machine Intelligence Special Issue on Bayesian Nonparametrics.  Vol. 37(2), pp. 462-474.</div>
<div>[<a data-toggle="collapse" href="#abs_Palla2015">Abstract</a>][<a href="#bib_Palla2015" data-toggle="collapse">BibTeX</a>] [<a href="https://doi.org/10.1109/TPAMI.2014.2324586" target="_blank">DOI</a>] [<a href="//mlg.eng.cam.ac.uk/konstantina/ILA/irmmulti.pdf" target="_blank">PDF</a>]</div>

<div id="abs_Palla2015" class="collapse">
	<b>Abstract</b>: Latent variable models for network data extract a summary of the relational structure underlying an observed network. The simplest possible models subdivide nodes of the network into clusters; the probability of a link between any two nodes then depends only on their cluster assignment. Currently available models can be classified by whether clusters are disjoint or are allowed to overlap. These models can explain a flat clustering structure. Hierarchical Bayesian models provide a natural approach to capture more complex dependencies. We propose a model in which objects are characterised by a latent feature vector. Each feature is itself partitioned into disjoint groups (subclusters), corresponding to a second layer of hierarchy. In experimental comparisons, the model achieves significantly improved predictive performance on social and biological link prediction tasks. The results indicate that models with a single layer hierarchy over-simplify real networks.
</div>
<div id="bib_Palla2015" class="collapse">
<b>BibTeX</b>:
<pre>
@article{Palla2015,
  author = {Palla, Konstantina and Knowles, David A. and Ghahramani, Zoubin},
  title = {Relational learning and network modelling using infinite latent attribute models},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence Special Issue on Bayesian Nonparametrics},
  year = {2015},
  volume = {37},
  number = {2},
  pages = {462--474},
  doi = {10.1109/TPAMI.2014.2324586}
}
</pre>
</div>
</li><li><div id="Shah2015"><em>Shah A, Knowles DA and Ghahramani Z</em> (2015), <strong>"An Empirical Study of Stochastic Variational Inference Algorithms for the Beta Bernoulli Process"</strong>, In Proceedings of the 32nd International Conference on Machine Learning. , pp. 1594-1603.</div>
<div>[<a data-toggle="collapse" href="#abs_Shah2015">Abstract</a>][<a href="#bib_Shah2015" data-toggle="collapse">BibTeX</a>] [<a href="http://proceedings.mlr.press/v37/shahb15.pdf" target="_blank">URL</a>]</div>

<div id="abs_Shah2015" class="collapse">
	<b>Abstract</b>: Stochastic variational inference (SVI) is emerging as the most promising candidate for scaling inference in Bayesian probabilistic models to large datasets. However, the performance of these methods has been assessed primarily in the context of Bayesian topic models, particularly latent Dirichlet allocation (LDA). Deriving several new algorithms, and using synthetic, image and genomic datasets, we investigate whether the understanding gleaned from LDA applies in the setting of sparse latent factor models, specifically beta process factor analysis (BPFA). We demonstrate that the big picture is consistent: using Gibbs sampling within SVI to maintain certain posterior dependencies is extremely effective. However, we find that different posterior dependencies are important in BPFA relative to LDA. Particularly, approximations able to model intra-local variable dependence perform best.
</div>
<div id="bib_Shah2015" class="collapse">
<b>BibTeX</b>:
<pre>
@inproceedings{Shah2015,
  author = {Shah, Amar and Knowles, David A and Ghahramani, Zoubin},
  title = {An Empirical Study of Stochastic Variational Inference Algorithms for the Beta Bernoulli Process},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  year = {2015},
  pages = {1594--1603},
  url = {http://proceedings.mlr.press/v37/shahb15.pdf}
}
</pre>
</div>
</li><li><div id="Heaukulani2014beta"><em>Heaukulani C, Knowles DA and Ghahramani Z</em> (2014), <strong>"Beta Diffusion Trees"</strong>, In Proceedings of the 31st International Conference on Machine Learning. , pp. 1809-1817.</div>
<div>[<a data-toggle="collapse" href="#abs_Heaukulani2014beta">Abstract</a>][<a href="#bib_Heaukulani2014beta" data-toggle="collapse">BibTeX</a>] [<a href="http://proceedings.mlr.press/v32/heaukulani14.pdf" target="_blank">URL</a>]</div>

<div id="abs_Heaukulani2014beta" class="collapse">
	<b>Abstract</b>: We define the beta diffusion tree, a random tree structure with a set of leaves that defines a collection of overlapping subsets of objects, known as a feature allocation. A generative process for the tree structure is defined in terms of particles (representing the objects) diffusing in some continuous space, analogously to the Dirichlet diffusion tree (Neal, 2003b), which defines a tree structure over partitions (i.e., non-overlapping subsets) of the objects. Unlike in the Dirichlet diffusion tree, multiple copies of a particle may exist and diffuse along multiple branches in the beta diffusion tree, and an object may therefore belong to multiple subsets of particles. We demonstrate how to build a hierarchically-clustered factor analysis model with the beta diffusion tree and how to perform inference over the random tree structures with a Markov chain Monte Carlo algorithm. We conclude with several numerical experiments on missing data problems with data sets of gene expression microarrays, international development statistics, and intranational socioeconomic measurements.
</div>
<div id="bib_Heaukulani2014beta" class="collapse">
<b>BibTeX</b>:
<pre>
@inproceedings{Heaukulani2014beta,
  author = {Heaukulani, Creighton and Knowles, David A. and Ghahramani, Zoubin},
  title = {Beta Diffusion Trees},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  year = {2014},
  pages = {1809--1817},
  url = {http://proceedings.mlr.press/v32/heaukulani14.pdf}
}
</pre>
</div>
</li><li><div id="Knowles2014a"><em>Knowles DA, Palla K and Ghahramani Z</em> (2014), <strong>"A reversible infinite HMM using normalised random measures"</strong>, In Proceedings of The 31st International Conference on Machine Learning. </div>
<div>[<a data-toggle="collapse" href="#abs_Knowles2014a">Abstract</a>][<a href="#bib_Knowles2014a" data-toggle="collapse">BibTeX</a>] [<a href="http://proceedings.mlr.press/v32/knowles14.pdf" target="_blank">URL</a>]</div>

<div id="abs_Knowles2014a" class="collapse">
	<b>Abstract</b>: We present a nonparametric prior over reversible Markov chains. We use completely random measures, specifically gamma processes, to construct a countably infinite graph with weighted edges. By enforcing symmetry to make the edges undirected we define a prior over random walks on graphs that results in a reversible Markov chain. The resulting prior over infinite transition matrices is closely related to the hierarchical Dirichlet process but enforces reversibility. A reinforcement scheme has recently been proposed with similar properties, but the de Finetti measure is not well characterised. We take the alternative approach of explicitly constructing the mixing measure, which allows more straightforward and efficient inference at the cost of no longer having a closed form predictive distribution. We use our process to construct a reversible infinite HMM which we apply to two real datasets, one from epigenomics and one ion channel recording.
</div>
<div id="bib_Knowles2014a" class="collapse">
<b>BibTeX</b>:
<pre>
@inproceedings{Knowles2014a,
  author = {Knowles, David A. and Palla, Konstantina and Ghahramani, Zoubin},
  title = {A reversible infinite HMM using normalised random measures},
  booktitle = {Proceedings of The 31st International Conference on Machine Learning},
  year = {2014},
  url = {http://proceedings.mlr.press/v32/knowles14.pdf}
}
</pre>
</div>
</li><li><div id="quadrianto2013supervised"><em>Quadrianto N, Sharmanska V, Knowles DA and Ghahramani Z</em> (2013), <strong>"The Supervised IBP: Neighbourhood Preserving Infinite Latent Feature Models"</strong>, In Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence. </div>
<div>[<a data-toggle="collapse" href="#abs_quadrianto2013supervised">Abstract</a>][<a href="#bib_quadrianto2013supervised" data-toggle="collapse">BibTeX</a>] [<a href="http://mlg.eng.cam.ac.uk/pub/pdf/QuaShaKnoGha13.pdf" target="_blank">URL</a>]</div>

<div id="abs_quadrianto2013supervised" class="collapse">
	<b>Abstract</b>: We propose a probabilistic model to infer supervised latent variables in the Hamming space from observed data. Our model allows simultaneous inference of the number of binary latent variables, and their values. The latent variables preserve neighbourhood structure of the data in a sense that objects in the same semantic concept have similar latent values, and objects in different concepts have dissimilar latent values. We formulate the supervised infinite latent variable problem based on an intuitive principle of pulling objects together if they are of the same type, and pushing them apart if they are not. We then combine this principle with a flexible Indian Buffet Process prior on the latent variables. We show that the inferred supervised latent variables can be directly used to perform a nearest neighbour search for the purpose of retrieval. We introduce a new application of dynamically extending hash codes, and show how to effectively couple the structure of the hash codes with continuously growing structure of the neighbourhood preserving infinite latent feature space.
</div>
<div id="bib_quadrianto2013supervised" class="collapse">
<b>BibTeX</b>:
<pre>
@inproceedings{quadrianto2013supervised,
  author = {Quadrianto, Novi and Sharmanska, Viktoriia and Knowles, David A and Ghahramani, Zoubin},
  title = {The Supervised IBP: Neighbourhood Preserving Infinite Latent Feature Models},
  booktitle = {Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence},
  year = {2013},
  url = {http://mlg.eng.cam.ac.uk/pub/pdf/QuaShaKnoGha13.pdf}
}
</pre>
</div>
</li><li><div id="salimans2013"><em>Salimans T and Knowles DA</em> (2013), <strong>"Fixed-form variational posterior approximation through stochastic linear regression"</strong>, Bayesian Analysis.  Vol. 8(4), pp. 837-882. Winner of the International Society for Bayesian Analysis Lindley Prize..</div>
<div>[<a data-toggle="collapse" href="#abs_salimans2013">Abstract</a>][<a href="#bib_salimans2013" data-toggle="collapse">BibTeX</a>] [<a href="https://doi.org/10.1214/13-BA858" target="_blank">DOI</a>] [<a href="http://projecteuclid.org/euclid.ba/1386166315" target="_blank">URL</a>]</div>

<div id="abs_salimans2013" class="collapse">
	<b>Abstract</b>: We propose a general algorithm for approximating nonstandard Bayesian posterior distributions. The algorithm minimizes the Kullback-Leibler divergence of an approximating distribution to the intractable posterior distribution. Our method can be used to approximate any posterior distribution, provided that it is given in closed form up to the proportionality constant. The approximation can be any distribution in the exponential family or any mixture of such distributions, which means that it can be made arbitrarily precise. Several examples illustrate the speed and accuracy of our approximation method in practice.
</div>
<div id="bib_salimans2013" class="collapse">
<b>BibTeX</b>:
<pre>
@article{salimans2013,
  author = {Salimans, Tim and Knowles, David A.},
  title = {Fixed-form variational posterior approximation through stochastic linear regression},
  journal = {Bayesian Analysis},
  publisher = {International Society for Bayesian Analysis},
  year = {2013},
  volume = {8},
  number = {4},
  pages = {837--882},
  url = {http://projecteuclid.org/euclid.ba/1386166315},
  doi = {10.1214/13-BA858}
}
</pre>
</div>
</li><li><div id="palla2012infinite"><em>Palla* K, Knowles* DA and Ghahramani Z</em> (2012), <strong>"An Infinite Latent Attribute Model for Network Data"</strong>, In Proceedings of the 29th International Conference on Machine Learning. , pp. 1607-1614. *These authors contributed equally to this work.</div>
<div>[<a data-toggle="collapse" href="#abs_palla2012infinite">Abstract</a>][<a href="#bib_palla2012infinite" data-toggle="collapse">BibTeX</a>] [<a href="http://icml.cc/2012/papers/785.pdf" target="_blank">URL</a>]</div>

<div id="abs_palla2012infinite" class="collapse">
	<b>Abstract</b>: Latent variable models for network data extract a summary of the relational structure underlying an observed network. The simplest possible models subdivide nodes of the network into clusters; the probability of a link between any two nodes then depends only on their cluster assignment. Currently available models can be classified by whether clusters are disjoint or are allowed to overlap. These models can explain clustering structure. Hierarchical Bayesian models provide a natural approach to capture more complex dependencies. We propose a model in which objects are characterised by a latent feature vector. Each feature is itself partitioned into disjoint groups (subclusters), corresponding to a second layer of hierarchy. In experimental comparisons, the model achieves significantly improved predictive performance on social and biological link prediction tasks. The results indicate that models with a single layer hierarchy over-simplify real networks.
</div>
<div id="bib_palla2012infinite" class="collapse">
<b>BibTeX</b>:
<pre>
@inproceedings{palla2012infinite,
  author = {Palla*, Konstantina and Knowles*, David A. and Ghahramani, Zoubin},
  title = {An Infinite Latent Attribute Model for Network Data},
  booktitle = {Proceedings of the 29th International Conference on Machine Learning},
  year = {2012},
  pages = {1607--1614},
  url = {http://icml.cc/2012/papers/785.pdf}
}
</pre>
</div>
</li><li><div id="Palla2012nonparametric"><em>Palla* K, Knowles* DA and Ghahramani Z</em> (2012), <strong>"A nonparametric variable clustering model"</strong>, In Advances in Neural Information Processing Systems.  Vol. 5, pp. 2987-2995. *These authors contributed equally to this work.</div>
<div>[<a data-toggle="collapse" href="#abs_Palla2012nonparametric">Abstract</a>][<a href="#bib_Palla2012nonparametric" data-toggle="collapse">BibTeX</a>] [<a href="https://papers.nips.cc/paper/4579-a-nonparametric-variable-clustering-model" target="_blank">URL</a>] [<a href="https://code.google.com/p/dpvc/" target="_blank">Code</a>]</div>

<div id="abs_Palla2012nonparametric" class="collapse">
	<b>Abstract</b>: Factor analysis models effectively summarise the covariance structure of high dimensional data, but the solutions are typically hard to interpret. This motivates attempting to find a disjoint partition, i.e. a clustering, of observed variables so that variables in a cluster are highly correlated. We introduce a Bayesian non-parametric approach to this problem, and demonstrate advantages over heuristic methods proposed to date.
</div>
<div id="bib_Palla2012nonparametric" class="collapse">
<b>BibTeX</b>:
<pre>
@inproceedings{Palla2012nonparametric,
  author = {Palla*, Konstantina and Knowles*, David A. and Ghahramani, Zoubin},
  title = {A nonparametric variable clustering model},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2012},
  volume = {5},
  pages = {2987--2995},
  url = {https://papers.nips.cc/paper/4579-a-nonparametric-variable-clustering-model}
}
</pre>
</div>
</li><li><div id="wilson2011gaussian"><em>Wilson AG, Knowles DA and Ghahramani Z</em> (2012), <strong>"Gaussian Process Regression Networks"</strong>, In Proceedings of the 29th International Conference on Machine Learning. , pp. 599-606.</div>
<div>[<a data-toggle="collapse" href="#abs_wilson2011gaussian">Abstract</a>][<a href="#bib_wilson2011gaussian" data-toggle="collapse">BibTeX</a>] [<a href="http://icml.cc/2012/papers/329.pdf" target="_blank">URL</a>] [<a href="https://github.com/davidaknowles/gprn" target="_blank">Code</a>]</div>

<div id="abs_wilson2011gaussian" class="collapse">
	<b>Abstract</b>: We introduce a new regression frame- work, Gaussian process regression networks (GPRN), which combines the structural properties of Bayesian neural networks with the nonparametric flexibility of Gaussian pro- cesses. GPRN accommodates input (pre- dictor) dependent signal and noise corre- lations between multiple output (response) variables, input dependent length-scales and amplitudes, and heavy-tailed predictive dis- tributions. We derive both elliptical slice sampling and variational Bayes inference pro- cedures for GPRN. We apply GPRN as a multiple output regression and multivariate volatility model, demonstrating substantially improved performance over eight popular multiple output (multi-task) Gaussian pro- cess models and three multivariate volatility models on real datasets, including a 1000 di- mensional gene expression dataset.
</div>
<div id="bib_wilson2011gaussian" class="collapse">
<b>BibTeX</b>:
<pre>
@inproceedings{wilson2011gaussian,
  author = {Wilson, Andrew Gordon and Knowles, David A. and Ghahramani, Zoubin},
  title = {Gaussian Process Regression Networks},
  booktitle = {Proceedings of the 29th International Conference on Machine Learning},
  year = {2012},
  pages = {599--606},
  url = {http://icml.cc/2012/papers/329.pdf}
}
</pre>
</div>
</li><li><div id="Gael2011"><em>Knowles DA, Gael JV and Ghahramani Z</em> (2011), <strong>"Message Passing Algorithms for the Dirichlet Diffusion Tree"</strong>, In Proceedings of the 28th International Conference on Machine Learning. , pp. 721-728.</div>
<div>[<a data-toggle="collapse" href="#abs_Gael2011">Abstract</a>][<a href="#bib_Gael2011" data-toggle="collapse">BibTeX</a>] [<a href="http://www.icml-2011.org/papers/410_icmlpaper.pdf" target="_blank">URL</a>]</div>

<div id="abs_Gael2011" class="collapse">
	<b>Abstract</b>: We demonstrate efficient approximate inference for the Dirichlet Diffusion Tree (Neal, 2003), a Bayesian nonparametric prior over tree structures. Although DDTs provide a powerful and elegant approach for modeling hierarchies they haven't seen much use to date. One problem is the computational cost of MCMC inference. We provide the first deterministic approximate inference methods for DDT models and show excellent performance compared to the MCMC alternative. We present message passing algorithms to approximate the Bayesian model evidence for a specific tree. This is used to drive sequential tree building and greedy search to find optimal tree structures, corresponding to hierarchical clusterings of the data. We demonstrate appropriate observation models for continuous and binary data. The empirical performance of our method is very close to the computationally expensive MCMC alternative on a density estimation problem, and significantly outperforms kernel density estimators.
</div>
<div id="bib_Gael2011" class="collapse">
<b>BibTeX</b>:
<pre>
@inproceedings{Gael2011,
  author = {Knowles, David A. and Gael, Jurgen Van and Ghahramani, Zoubin},
  title = {Message Passing Algorithms for the Dirichlet Diffusion Tree},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning},
  year = {2011},
  pages = {721--728},
  url = {http://www.icml-2011.org/papers/410_icmlpaper.pdf}
}
</pre>
</div>
</li><li><div id="Knowles2011b"><em>Knowles DA and Ghahramani Z</em> (2011), <strong>"Pitman-Yor Diffusion Trees"</strong>, In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence. , pp. 410-418.</div>
<div>[<a data-toggle="collapse" href="#abs_Knowles2011b">Abstract</a>][<a href="#bib_Knowles2011b" data-toggle="collapse">BibTeX</a>] [<a href="http://dl.acm.org/citation.cfm?id=3020596" target="_blank">URL</a>] [<a href="//mlg.eng.cam.ac.uk/pub/pdf/KnoGha11a.pdf" target="_blank">PDF</a>]</div>

<div id="abs_Knowles2011b" class="collapse">
	<b>Abstract</b>: We introduce the Pitman Yor Diffusion Tree (PYDT) for hierarchical clustering, a generalization of the Dirichlet Diffusion Tree (Neal, 2001) which removes the restriction to binary branching structure. The generative process is described and shown to result in an exchangeable distribution over data points. We prove some theoretical properties of the model and then present two inference methods: a collapsed MCMC sampler which allows us to model uncertainty over tree structures, and a computationally efficient greedy Bayesian EM search algorithm. Both algorithms use message passing on the tree structure. The utility of the model and algorithms is demonstrated on synthetic and real world data, both continuous and binary.
</div>
<div id="bib_Knowles2011b" class="collapse">
<b>BibTeX</b>:
<pre>
@inproceedings{Knowles2011b,
  author = {Knowles, David A. and Ghahramani, Zoubin},
  title = {Pitman-Yor Diffusion Trees},
  booktitle = {Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence},
  year = {2011},
  pages = {410--418},
  url = {http://dl.acm.org/citation.cfm?id=3020596}
}
</pre>
</div>
</li><li><div id="Knowles2011c"><em>Knowles DA and Minka T</em> (2011), <strong>"Non-conjugate Variational Message Passing for Multinomial and Binary Regression"</strong>, In Advances in Neural Information Processing Systems. , pp. 1701-1709.</div>
<div>[<a data-toggle="collapse" href="#abs_Knowles2011c">Abstract</a>][<a href="#bib_Knowles2011c" data-toggle="collapse">BibTeX</a>] [<a href="http://papers.nips.cc/paper/4407-non-conjugate-variational-message-passing-for-multinomial-and-binary-regression" target="_blank">URL</a>]</div>

<div id="abs_Knowles2011c" class="collapse">
	<b>Abstract</b>: Variational Message Passing (VMP) is an algorithmic implementation of the Variational Bayes (VB) method which applies only in the special case of conjugate exponential family models. We propose an extension to VMP, which we refer to as Non-conjugate Variational Message Passing (NCVMP) which aims to alleviate this restriction while maintaining modularity, allowing choice in how expectations are calculated, and integrating into an existing message-passing framework: Infer.NET. We demonstrate NCVMP on logistic binary and multinomial regression. In the multinomial case we introduce a novel variational bound for the softmax factor which is tighter than other commonly used bounds whilst maintaining computational tractability.
</div>
<div id="bib_Knowles2011c" class="collapse">
<b>BibTeX</b>:
<pre>
@inproceedings{Knowles2011c,
  author = {Knowles, David A. and Minka, Tom},
  title = {Non-conjugate Variational Message Passing for Multinomial and Binary Regression},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2011},
  pages = {1701--1709},
  url = {http://papers.nips.cc/paper/4407-non-conjugate-variational-message-passing-for-multinomial-and-binary-regression}
}
</pre>
</div>
</li><li><div id="Knowles2011nonparametric"><em>Knowles DA and Ghahramani Z</em> (2011), <strong>"Nonparametric Bayesian sparse factor models with application to gene expression modeling"</strong>, The Annals of Applied Statistics.  Vol. 5(2B), pp. 1534-1552.</div>
<div>[<a data-toggle="collapse" href="#abs_Knowles2011nonparametric">Abstract</a>][<a href="#bib_Knowles2011nonparametric" data-toggle="collapse">BibTeX</a>] [<a href="https://doi.org/10.1214/10-AOAS435" target="_blank">DOI</a>] [<a href="https://projecteuclid.org/euclid.aoas/1310562732" target="_blank">URL</a>] [<a href="//mlg.eng.cam.ac.uk/pub/pdf/KnoGha11b.pdf" target="_blank">PDF</a>] [<a href="http://nsfa.googlecode.com" target="_blank">Code</a>]</div>

<div id="abs_Knowles2011nonparametric" class="collapse">
	<b>Abstract</b>: A nonparametric Bayesian extension of Factor Analysis (FA) is proposed where observed data Y is modeled as a linear superposition, G, of a potentially infinite number of hidden factors, X. The Indian Buffet Process (IBP) is used as a prior on G to incorporate sparsity and to allow the number of latent features to be inferred. The model's utility for modeling gene expression data is investigated using randomly generated datasets based on a known sparse connectivity matrix for E. Coli, and on three biological datasets of increasing complexity.
</div>
<div id="bib_Knowles2011nonparametric" class="collapse">
<b>BibTeX</b>:
<pre>
@article{Knowles2011nonparametric,
  author = {Knowles, David A. and Ghahramani, Zoubin},
  title = {Nonparametric Bayesian sparse factor models with application to gene expression modeling},
  journal = {The Annals of Applied Statistics},
  publisher = {Institute of Mathematical Statistics},
  year = {2011},
  volume = {5},
  number = {2B},
  pages = {1534--1552},
  url = {https://projecteuclid.org/euclid.aoas/1310562732},
  doi = {10.1214/10-AOAS435}
}
</pre>
</div>
</li><li><div id="Doshi-velez2009"><em>Doshi-Velez* F, Mohamed* S, Knowles* DA and Ghahramani Z</em> (2009), <strong>"Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process"</strong>, In Advances in Neural Information Processing Systems. , pp. 1294-1302. *These authors contributed equally to this work.</div>
<div>[<a data-toggle="collapse" href="#abs_Doshi-velez2009">Abstract</a>][<a href="#bib_Doshi-velez2009" data-toggle="collapse">BibTeX</a>] [<a href="http://papers.nips.cc/paper/3669-large-scale-nonparametric-bayesian-inference-data-parallelisation-in-the-indian-buffet-process" target="_blank">URL</a>]</div>

<div id="abs_Doshi-velez2009" class="collapse">
	<b>Abstract</b>: Nonparametric Bayesian models provide a framework for flexible probabilistic modelling of complex datasets. Unfortunately, Bayesian inference methods often require high-dimensional averages and can be slow to compute, especially with the potentially unbounded representations associated with nonparametric models. We address the challenge of scaling nonparametric Bayesian inference to the increasingly large datasets found in real-world applications, focusing on the case of parallelising inference in the Indian Buffet Process (IBP). Our approach divides a large data set between multiple processors. The processors use message passing to compute likelihoods in an asynchronous, distributed fashion and to propagate statistics about the global Bayesian posterior. This novel MCMC sampler is the first parallel inference scheme for IBP-based models, scaling to datasets orders of magnitude larger than had previously been possible.
</div>
<div id="bib_Doshi-velez2009" class="collapse">
<b>BibTeX</b>:
<pre>
@inproceedings{Doshi-velez2009,
  author = {Doshi-Velez*, Finale and Mohamed*, Shakir and Knowles*, David A. and Ghahramani, Zoubin},
  title = {Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2009},
  pages = {1294--1302},
  url = {http://papers.nips.cc/paper/3669-large-scale-nonparametric-bayesian-inference-data-parallelisation-in-the-indian-buffet-process}
}
</pre>
</div>
</li><li><div id="Knowles07iica"><em>Knowles DA and Ghahramani Z</em> (2007), <strong>"Infinite Sparse Factor Analysis and Infinite Independent Components Analysis"</strong>, In 7th International Conference on Independent Component Analysis and Signal Separation. </div>
<div>[<a data-toggle="collapse" href="#abs_Knowles07iica">Abstract</a>][<a href="#bib_Knowles07iica" data-toggle="collapse">BibTeX</a>] [<a href="https://doi.org/10.1007/978-3-540-74494-8" target="_blank">DOI</a>] [<a href="http://www.springerlink.com/index/10.1007/978-3-540-74494-8" target="_blank">URL</a>] [<a href="//mlg.eng.cam.ac.uk/pub/pdf/KnoGha07.pdf" target="_blank">PDF</a>]</div>

<div id="abs_Knowles07iica" class="collapse">
	<b>Abstract</b>: A nonparametric Bayesian extension of Independent Components Analysis (ICA) is proposed where observed data Y is modelled as a linear superposition, G, of a potentially infinite number of hidden sources, X. Whether a given source is active for a specific data point is specified by an infinite binary matrix, Z. The resulting sparse representation allows increased data reduction compared to standard ICA. We define a prior on Z using the Indian Buffet Process (IBP). We describe four variants of the model, with Gaussian or Laplacian priors on X and the one or two-parameter IBPs. We demonstrate Bayesian inference under these models using a Markov Chain Monte Carlo (MCMC) algorithm on synthetic and gene expression data and compare to standard ICA algorithms.
</div>
<div id="bib_Knowles07iica" class="collapse">
<b>BibTeX</b>:
<pre>
@inproceedings{Knowles07iica,
  author = {Knowles, David A. and Ghahramani, Zoubin},
  title = {Infinite Sparse Factor Analysis and Infinite Independent Components Analysis},
  booktitle = {7th International Conference on Independent Component Analysis and Signal Separation},
  year = {2007},
  url = {http://www.springerlink.com/index/10.1007/978-3-540-74494-8},
  doi = {10.1007/978-3-540-74494-8}
}
</pre>
</div>
</li></ol>
<ol>
<li><div id="Su2022"><em>Su J, Reynier J-B, Fu X, Zhong G, Jiang J, Escalante RS, Wang Y, Izar B, Knowles DA and Rabadan R</em> (2022), <strong>"A Unified Modular Framework to Incorporate Structural Dependency in Spatial Omics Data"</strong>, bioRxiv. , pp. 2022.10.25.513785.</div>
<div>[<a data-toggle="collapse" href="#abs_Su2022">Abstract</a>][<a href="#bib_Su2022" data-toggle="collapse">BibTeX</a>] [<a href="https://www.biorxiv.org/content/10.1101/2022.10.25.513785v1.abstract" target="_blank">URL</a>]</div>

<div id="abs_Su2022" class="collapse">
	<b>Abstract</b>: Spatial omics technologies, such as spatial transcriptomics, allow the identification of spatially organized biological processes, while presenting computational challenges for existing analysis approaches that ignore spatial dependencies. Here we introduce Smoother, a unified and modular framework that integrates positional information into non-spatial models via spatial priors and losses. In simulated and real datasets, we show that Smoother enables spatially aware data imputation, cell-type deconvolution, and dimensionality reduction with high accuracy.
</div>
<div id="bib_Su2022" class="collapse">
<b>BibTeX</b>:
<pre>
@article{Su2022,
  author = {Su, Jiayu and Reynier, Jean-Baptiste and Fu, Xi and Zhong, Guojie and Jiang, Jiahao and Escalante, Rydberg Supo and Wang, Yiping and Izar, Benjamin and Knowles, David A and Rabadan, Raul},
  title = {A Unified Modular Framework to Incorporate Structural Dependency in Spatial Omics Data},
  journal = {bioRxiv},
  year = {2022},
  pages = {2022.10.25.513785},
  url = {https://www.biorxiv.org/content/10.1101/2022.10.25.513785v1.abstract}
}
</pre>
</div>
</li><li><div id="Brown2020"><em>Brown BC and Knowles DA</em> (2020), <strong>"Phenome-scale causal network discovery with bidirectional mediated Mendelian randomization"</strong>, bioRxiv. </div>
<div>[<a data-toggle="collapse" href="#abs_Brown2020">Abstract</a>][<a href="#bib_Brown2020" data-toggle="collapse">BibTeX</a>] [<a href="https://doi.org/10.1101/2020.06.18.160176" target="_blank">DOI</a>] [<a href="https://www.biorxiv.org/content/early/2020/06/22/2020.06.18.160176" target="_blank">URL</a>]</div>

<div id="abs_Brown2020" class="collapse">
	<b>Abstract</b>: Inference of directed biological networks from observational genomics datasets is a crucial but notoriously difficult challenge. Modern population-scale biobanks, containing simultaneous measurements of traits, biomarkers, and genetic variation, offer an unprecedented opportunity to study biological networks. Mendelian randomization (MR) has received attention as a class of methods for inferring causal effects in observational data that uses genetic variants as instrumental variables, but MR methods rely on assumptions that limit their application to complex traits at the biobank-scale. Moreover, MR estimates the total effect of one trait on another, which may be mediated by other factors. Biobanks include measurements of many potential mediators, in principle enabling the conversion of MR estimates into direct effects representing a causal network. Here, we show that this can be accomplished by a flexible two stage procedure we call bidirectional mediated Mendelian randomization (bimmer). First, bimmer estimates the effect of every trait on every other. Next, bimmer finds a parsimonious network that explains these effects using direct and mediated causal paths. We introduce novel methods for both steps and show via extensive simulations that bimmer is able to learn causal network structures even in the presence of non-causal genetic correlation. We apply bimmer to 405 phenotypes from the UK biobank and demonstrate that learning the network structure is invaluable for interpreting the results of phenome-wide MR, while lending causal support to several recent observational studies.
</div>
<div id="bib_Brown2020" class="collapse">
<b>BibTeX</b>:
<pre>
@article{Brown2020,
  author = {Brown, Brielin C. and Knowles, David A.},
  title = {Phenome-scale causal network discovery with bidirectional mediated Mendelian randomization},
  journal = {bioRxiv},
  publisher = {Cold Spring Harbor Laboratory},
  year = {2020},
  url = {https://www.biorxiv.org/content/early/2020/06/22/2020.06.18.160176},
  doi = {10.1101/2020.06.18.160176}
}
</pre>
</div>
</li><li><div id="stirn2020variational"><em>Stirn A and Knowles DA</em> (2020), <strong>"Variational Variance: Simple and Reliable Predictive Variance Parameterization"</strong>, arXiv. </div>
<div>[<a data-toggle="collapse" href="#abs_stirn2020variational">Abstract</a>][<a href="#bib_stirn2020variational" data-toggle="collapse">BibTeX</a>] [<a href="arXiv:2006.04910v2" target="_blank">DOI</a>] [<a href="https://arxiv.org/abs/2006.04910" target="_blank">URL</a>]</div>

<div id="abs_stirn2020variational" class="collapse">
	<b>Abstract</b>: Brittle optimization has been observed to adversely impact model likelihoods for regression and VAEs when simultaneously fitting neural network mappings from a (random) variable onto the mean and variance of a dependent Gaussian variable. Previous works have bolstered optimization and improved likelihoods, but fail other basic posterior predictive checks (PPCs). Under the PPC framework, we propose critiques to test predictive mean and variance calibration and the predictive distribution's ability to generate sensible data. We find that our attractively simple solution, to treat heteroscedastic variance variationally, sufficiently regularizes variance to pass these PPCs. We consider a diverse gamut of existing and novel priors and find our methods preserve or outperform existing model likelihoods while significantly improving parameter calibration and sample quality for regression and VAEs.
</div>
<div id="bib_stirn2020variational" class="collapse">
<b>BibTeX</b>:
<pre>
@article{stirn2020variational,
  author = {Stirn, Andrew and Knowles, David A},
  title = {Variational Variance: Simple and Reliable Predictive Variance Parameterization},
  journal = {arXiv},
  year = {2020},
  url = {https://arxiv.org/abs/2006.04910},
  doi = {arXiv:2006.04910v2}
}
</pre>
</div>
</li><li><div id="Knowles2015stochastic"><em>Knowles DA</em> (2015), <strong>"Stochastic gradient variational Bayes for gamma approximating distributions"</strong>, arXiv. , pp. 1509.01631.</div>
<div>[<a data-toggle="collapse" href="#abs_Knowles2015stochastic">Abstract</a>][<a href="#bib_Knowles2015stochastic" data-toggle="collapse">BibTeX</a>] [<a href="https://arxiv.org/abs/1509.01631" target="_blank">URL</a>] [<a href="https://github.com/davidaknowles/gamma_sgvb" target="_blank">Code</a>]</div>

<div id="abs_Knowles2015stochastic" class="collapse">
	<b>Abstract</b>: While stochastic variational inference is relatively well known for scaling inference in Bayesian probabilistic models, related methods also offer ways to circumnavigate the approximation of analytically intractable expectations. The key challenge in either setting is controlling the variance of gradient estimates: recent work has shown that for continuous latent variables, particularly multivariate Gaussians, this can be achieved by using the gradient of the log posterior. In this paper we apply the same idea to gamma distributed latent variables given gamma variational distributions, enabling straightforward "black box" variational inference in models where sparsity and non-negativity are appropriate. We demonstrate the method on a recently proposed gamma process model for network data, as well as a novel sparse factor analysis. We outperform generic sampling algorithms and the approach of using Gaussian variational distributions on transformed variables.
</div>
<div id="bib_Knowles2015stochastic" class="collapse">
<b>BibTeX</b>:
<pre>
@article{Knowles2015stochastic,
  author = {Knowles, David A},
  title = {Stochastic gradient variational Bayes for gamma approximating distributions},
  journal = {arXiv},
  year = {2015},
  pages = {1509.01631},
  url = {https://arxiv.org/abs/1509.01631}
}
</pre>
</div>
</li><li><div id="salimans2014using"><em>Salimans T and Knowles DA</em> (2014), <strong>"On using control variates with stochastic approximation for variational Bayes and its connection to stochastic linear regression"</strong>, arXiv. , pp. 1401.1022.</div>
<div>[<a data-toggle="collapse" href="#abs_salimans2014using">Abstract</a>][<a href="#bib_salimans2014using" data-toggle="collapse">BibTeX</a>] [<a href="https://arxiv.org/abs/1401.1022" target="_blank">URL</a>]</div>

<div id="abs_salimans2014using" class="collapse">
	<b>Abstract</b>: Recently, we and several other authors have written about the possibilities of using stochastic approximation techniques for fitting variational approximations to intractable Bayesian posterior distributions. Naive implementations of stochastic approximation suffer from high variance in this setting. Several authors have therefore suggested using control variates to reduce this variance, while we have taken a different but analogous approach to reducing the variance which we call stochastic linear regression. In this note we take the former perspective and derive the ideal set of control variates for stochastic approximation variational Bayes under a certain set of assumptions. We then show that using these control variates is closely related to using the stochastic linear regression approximation technique we proposed earlier. A simple example shows that our method for constructing control variates leads to stochastic estimators with much lower variance compared to other approaches.
</div>
<div id="bib_salimans2014using" class="collapse">
<b>BibTeX</b>:
<pre>
@article{salimans2014using,
  author = {Salimans, Tim and Knowles, David A},
  title = {On using control variates with stochastic approximation for variational Bayes and its connection to stochastic linear regression},
  journal = {arXiv},
  year = {2014},
  pages = {1401.1022},
  url = {https://arxiv.org/abs/1401.1022}
}
</pre>
</div>
</li><li><div id="palla2013dependent"><em>Palla* K, Knowles* DA and Ghahramani Z</em> (2013), <strong>"A dependent partition-valued process for multitask clustering and time evolving network modeling"</strong>, arXiv. , pp. 1303.3265. *These authors contributed equally to this work.</div>
<div>[<a data-toggle="collapse" href="#abs_palla2013dependent">Abstract</a>][<a href="#bib_palla2013dependent" data-toggle="collapse">BibTeX</a>] [<a href="https://arxiv.org/abs/1303.3265" target="_blank">URL</a>]</div>

<div id="abs_palla2013dependent" class="collapse">
	<b>Abstract</b>: The fundamental aim of clustering algorithms is to partition data points. We consider tasks where the discovered partition is allowed to vary with some covariate such as space or time. One approach would be to use fragmentation-coagulation processes, but these, being Markov processes, are restricted to linear or tree structured covariate spaces. We define a partition-valued process on an arbitrary covariate space using Gaussian processes. We use the process to construct a multitask clustering model which partitions datapoints in a similar way across multiple data sources, and a time series model of network data which allows cluster assignments to vary over time. We describe sampling algorithms for inference and apply our method to defining cancer subtypes based on different types of cellular characteristics, finding regulatory modules from gene expression data from multiple human populations, and discovering time varying community structure in a social network.
</div>
<div id="bib_palla2013dependent" class="collapse">
<b>BibTeX</b>:
<pre>
@article{palla2013dependent,
  author = {Palla*, Konstantina and Knowles*, David A and Ghahramani, Zoubin},
  title = {A dependent partition-valued process for multitask clustering and time evolving network modeling},
  journal = {arXiv},
  year = {2013},
  pages = {1303.3265},
  url = {https://arxiv.org/abs/1303.3265}
}
</pre>
</div>
</li></ol>